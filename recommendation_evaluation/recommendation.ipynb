{
 "cells": [
  {
   "cell_type": "code",
   "id": "55bc1f80",
   "metadata": {},
   "source": [
    "from socialvec.socialvec import SocialVec\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import ast\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4c10e2e",
   "metadata": {},
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import gdown"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5aae9993",
   "metadata": {},
   "source": [
    "# Read all in order to get the number of followers per account (in our database)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cd14a571-98a9-4710-a662-84f10f5aeacd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Google Drive file ID for all_usa_users.pkl\n",
    "file_id = '1flhTAfxODFHze1lmmgG9CB7LHnsFIhUd'\n",
    "\n",
    "# Construct the direct download URL\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "\n",
    "# Download the file using gdown\n",
    "output = \"all_usa_users.pkl\"\n",
    "\n",
    "\n",
    "if not os.path.exists(output):\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "df_all = pd.read_pickle(output, compression='gzip')    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14d6eb63",
   "metadata": {},
   "source": [
    "df_all = df_all[df_all['friends_ids'].notna()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65d1d48f",
   "metadata": {},
   "source": [
    "def parse_friends_ids(row):\n",
    "    friends_ids_str = row.get(\"friends_ids\")\n",
    "    if friends_ids_str is None or not isinstance(friends_ids_str, str):\n",
    "        return []  # Return an empty list if the value is None or not a string\n",
    "    try:\n",
    "        friends_ids_str = friends_ids_str.strip(\"'\")  # Remove the enclosing quotes\n",
    "        friends_ids = ast.literal_eval(friends_ids_str)  # Safely evaluate the string\n",
    "        if friends_ids and isinstance(friends_ids, list):\n",
    "            return friends_ids[0]  # Extract the inner list if it exists\n",
    "    except (SyntaxError, ValueError, TypeError):  # Catch potential parsing errors\n",
    "        return []  # Return an empty list if parsing fails\n",
    "    return []  # Return an empty list if the structure is unexpected"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1e6eb1f",
   "metadata": {},
   "source": [
    "# Parse the string to a list\n",
    "def parse_friends_ids(row):\n",
    "    friends_ids_str = row[\"friends_ids\"].strip(\"'\")\n",
    "    friends_ids = ast.literal_eval(friends_ids_str)  # Safely evaluate the string\n",
    "    if friends_ids:\n",
    "        return friends_ids[0]  # Extract the inner list if it exists\n",
    "    return []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d58f5dfd",
   "metadata": {},
   "source": [
    "# Create dataframe with count of followers per account\n",
    "\n",
    "df_all[\"friends_ids_parsed\"] = df_all.progress_apply(parse_friends_ids, axis=1)\n",
    "exploded_df = df_all.explode(\"friends_ids_parsed\")\n",
    "df_count = exploded_df.groupby(by=['friends_ids_parsed']).count()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c65c2efba5117f6d",
   "metadata": {},
   "source": [
    "# Initialize SV"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1f473b5",
   "metadata": {},
   "source": [
    "sv = SocialVec()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5bfa0036",
   "metadata": {},
   "source": [
    "file_id = '1hAK8XNlveGqGJB5VtLax9g6NiaHKsMh9'\n",
    "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "output = \"../data/Labeled_data.csv\"\n",
    "\n",
    "if not os.path.exists(output):\n",
    "    gdown.download(url, output, quiet=False)\n",
    "\n",
    "wiki_df = pd.read_csv(output)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e648f914",
   "metadata": {},
   "source": [
    "popular_df = pd.read_pickle(\"https://www.dropbox.com/scl/fi/mmzt5rd1o11a85fxuzczs/users_with_over_200_DETAILS.pkl?rlkey=q4wy51zpl5s4rqiagskygu2mg&st=0m95xyae&dl=1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b00d719-839c-409f-8316-8d87db387b6a",
   "metadata": {},
   "source": [
    "## Tidy up the data and merge"
   ]
  },
  {
   "cell_type": "code",
   "id": "2270c221",
   "metadata": {},
   "source": [
    "popular_df.drop(popular_df.columns[popular_df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "wiki_df.drop(columns=[col for col in wiki_df.columns if col in popular_df.columns], inplace=True)\n",
    "df = pd.merge(wiki_df, popular_df, left_on='twitter_screen_name', right_on='screen_name')\n",
    "df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n",
    "df_count = df_count[['friends_ids']]\n",
    "df_count.reset_index(inplace=True)\n",
    "df_count.rename(columns= {'friends_ids_parsed':'twitter_user_id'}, inplace=True)\n",
    "df = pd.merge(df, df_count, on='twitter_user_id')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e3a032a-efd0-47ec-83ac-74a43e4a7688",
   "metadata": {},
   "source": [
    "# Create Popular Accounts Excel based on specific topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad8af3a-5d06-49c8-a3d9-f516a61b6555",
   "metadata": {},
   "source": [
    "## Remove items with no dbpedia types"
   ]
  },
  {
   "cell_type": "code",
   "id": "80e7f0b2-0077-4785-800d-3ff32db1ea36",
   "metadata": {},
   "source": [
    "df['dbpedia_types_tokens_hier'] = df['dbpedia_types_tokens_hier'].progress_apply(lambda x: eval(x))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7d34580-2fa3-454c-be33-f96f38f0da77",
   "metadata": {},
   "source": [
    "df['token_len'] = df['dbpedia_types_tokens_hier'].apply(lambda x: len(x))\n",
    "\n",
    "print(df.shape[0])\n",
    "df = df[df['token_len']>0]\n",
    "print(df.shape[0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0d0128ab-ef82-42f8-bf31-5046fd2c4043",
   "metadata": {},
   "source": [
    "df['category'] = df['dbpedia_types_tokens_hier'].progress_apply(lambda x: max(x, key=x.get))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32e140fc",
   "metadata": {},
   "source": [
    "df['dbpedia_types_tokens'] = df['dbpedia_types_tokens'].progress_apply(lambda x: eval(x))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "087544d3",
   "metadata": {},
   "source": [
    "df.drop_duplicates(subset=['wikidata_label','twitter_name'], keep='first', inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df7a2116-4ad1-42ce-9686-2e2872f7ee22",
   "metadata": {},
   "source": [
    "## Create excel with the most popular accounts per requested topic \n",
    "* Now create the file based on the categories plus some additional according to the description\n",
    "* (based on wikidata keyword/ label and popularity in our US based dataset)"
   ]
  },
  {
   "cell_type": "code",
   "id": "eaf3c900-8394-40d3-a7b4-23237de1e739",
   "metadata": {},
   "source": [
    "category_counts = df[\"dbpedia_types_tokens\"].explode().value_counts().to_frame().reset_index()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08c65643-ead4-4f32-9d6b-1f62a9139d78",
   "metadata": {},
   "source": [
    "category_counts.to_excel('Wikidata_category_counts.xlsx')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d271f34b-a087-4c92-835d-141dbc86011f",
   "metadata": {},
   "source": [
    "if not os.path.exists(output):\n",
    "    gdown.download(url, output, quiet=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "27233e2a-5ecb-4026-9ccd-1a400b77a4c5",
   "metadata": {},
   "source": [
    "topics = [\n",
    "    \"MusicalArtist\", \"Actor\", \"Politician\", \"TelevisionShow\", \"TelevisionStation\",\n",
    "    \"Comedian\", \"SportsTeam\", \"Film\", \"TelevisionHost\", \"Athlete\",\n",
    "    \"Journalist\", \"Band\", \"Magazine\", \"Scientist\", \"Restaurant\",\n",
    "    \"SportsTeam\", \"Event\", \"BusinessPerson\", \"Economist\", \"Youtuber\",\n",
    "    \"Device\", \"Comic\", \"Sport\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0a4d598-3b2b-472f-be12-d37a90e68ce6",
   "metadata": {},
   "source": [
    "category_counts.iloc[200:300]['dbpedia_types_tokens'].values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b1f8fdbb-f678-495f-a5af-48c199998698",
   "metadata": {},
   "source": [
    "description_keywords = [\n",
    "    'car ','news','brand','company'\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25ef58d3",
   "metadata": {},
   "source": [
    "file_path = '../data/popular_accounts.xlsx'\n",
    "\n",
    "# Save DataFrames to different tabs\n",
    "with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "    for topic in topics:\n",
    "        filtered_df = df[df[\"dbpedia_types_tokens\"].apply(lambda tokens: topic in tokens)]\n",
    "        #filtered_df = df[df['category']==topic]\n",
    "        filtered_df = filtered_df.sort_values(by='friends_ids', ascending=False).head(100)\n",
    "        filtered_df.to_excel(writer, sheet_name=topic, index=False)\n",
    "        \n",
    "    filtered_df = df[(~df['wikidata_desc'].isna())&\n",
    "       ((df['wikidata_desc'].str.contains('fashion'))|(df['wikidata_desc'].str.contains('cloth')))&\n",
    "       (df['wikidata_desc'].str.contains('company'))]\n",
    "    filtered_df = filtered_df.sort_values(by='friends_ids', ascending=False).head(100)\n",
    "    filtered_df.to_excel(writer, sheet_name='fashion', index=False)\n",
    "\n",
    "    for keyword in description_keywords:\n",
    "        filtered_df = df[(~df['wikidata_desc'].isna())&\n",
    "           (df['wikidata_desc'].str.contains(keyword))]    \n",
    "        filtered_df = filtered_df.sort_values(by='friends_ids', ascending=False).head(100)\n",
    "        filtered_df.to_excel(writer, sheet_name=keyword, index=False)\n",
    "\n",
    "    filtered_df = df[(~df['wikidata_desc'].isna())&\n",
    "       ((df['wikidata_desc'].str.contains('park'))|(df['wikidata_desc'].str.contains('attraction')))]\n",
    "    filtered_df = filtered_df.sort_values(by='friends_ids', ascending=False).head(100)\n",
    "    filtered_df.to_excel(writer, sheet_name='attractions', index=False)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1a130143-cc46-4c4c-b2d0-912b7ea616df",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "55010824",
   "metadata": {},
   "source": [
    "# Recommendation Similarities\n",
    "\n",
    "Implementation of similarity according to SocialVec"
   ]
  },
  {
   "cell_type": "code",
   "id": "e6cbaced",
   "metadata": {},
   "source": [
    "def check_if_same_type(user_id,etype):\n",
    "    condition = (df['twitter_user_id'] == user_id) & \\\n",
    "            df['dbpedia_types_tokens'].apply(lambda x: etype in x)\n",
    "    return condition.any()\n",
    "\n",
    "def new_get_similar(input: str,etype: str,topn: int = 10):\n",
    "        \"\"\"\n",
    "        This function returns the topn similar entities for a given entity\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : twitter user id or username\n",
    "        by : 'userid', 'username' or vector default is username\n",
    "        topn : requested numner of similar entities\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Pandas dataframe with the top n similar entities details\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if isinstance(input, int) or (isinstance(input, str) and input.isdigit()):\n",
    "            input = sv.validate_userid(input)\n",
    "        elif isinstance(input, str):\n",
    "            input = sv.validate_username(input)\n",
    "        #else input is a 'vector'\n",
    "        #etype = sv.entities[sv.entities['twitter_id']==(input)]['final_type_merged'].iloc[0]\n",
    "        sim = sv.sv.wv.most_similar(input, topn=500)\n",
    "        similar = [user for user in sim if check_if_same_type(int(user[0]),etype)]\n",
    "        similar = pd.DataFrame(similar, columns=['twitter_id', 'similarity']).head(topn)\n",
    "        \n",
    "        return pd.merge(similar, sv.entities, on='twitter_id', how='left')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "72f34573",
   "metadata": {},
   "source": [
    "# Read all users again, now we will use them for testing the recomendation performance"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b4df2a88",
   "metadata": {},
   "source": [
    "df_all = df_all[df_all['friends_ids'].notna()]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0577cf8d",
   "metadata": {},
   "source": [
    "\n",
    "sample_size = 20000\n",
    "\n",
    "df_subset_users = df_all.sample(sample_size)\n",
    "df_subset_users[\"friends_ids\"] = df_subset_users.progress_apply(parse_friends_ids, axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f7cd0080",
   "metadata": {},
   "source": [
    "# Check similarities\n",
    "\n",
    "Next we read again the accounts from the excel, after marking manually which accounts to use under the column \"use\"\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "18fb73ff",
   "metadata": {},
   "source": [
    "# Use the ExcelFile class to read the Excel file\n",
    "excel_file = pd.ExcelFile(\"popular_accounts_1.xlsx\")\n",
    "\n",
    "# Get all sheet names\n",
    "sheet_names = excel_file.sheet_names\n",
    "print(sheet_names)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a61628c",
   "metadata": {},
   "source": [
    "category = 'TelevisionHost'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef02a0b5",
   "metadata": {},
   "source": [
    "df_rec = pd.read_excel(\"popular_accounts_1.xlsx\", sheet_name='TelevisionHost')\n",
    "relevant_accounts = df_rec[df_rec['use']==1]\n",
    "\n",
    "users_to_remove = []\n",
    "\n",
    "for i, ac in relevant_accounts.iterrows():\n",
    "    try:\n",
    "        sv[ac['twitter_user_id']]\n",
    "    except:\n",
    "        users_to_remove.append(ac['twitter_user_id'])\n",
    "\n",
    "        \n",
    "relevant_accounts = relevant_accounts[~relevant_accounts['twitter_user_id'].isin(users_to_remove)]\n",
    "\n",
    "print(f\"removed {users_to_remove} users\")\n",
    "print(f\"remaining {relevant_accounts.shape[0]} users\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab33ddbf",
   "metadata": {},
   "source": [
    "### Similarity-Based Recommendation Analysis\n",
    "\n",
    "The presented code segment evaluates the effectiveness of similarity-based recommendations for social media accounts, specifically analyzing Twitter user behavior. \n",
    "\n",
    "#### Experimental Setup\n",
    "- A subset consisting of **10,000 accounts** is selected from a larger dataset.\n",
    "- For each user, the algorithm isolates the accounts they follow (`friends_ids`) and identifies a subset after removing previously identified relevant accounts.\n",
    "\n",
    "#### Methodology\n",
    "- Embedding vectors from a predefined embedding space (`sv`) are utilized.\n",
    "- A user's representative vector is computed by averaging embeddings of followed but non-relevant accounts.\n",
    "- Cosine similarity is calculated between the user's representative vector and each relevant account's vector, resulting in a similarity-ranked list.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "- The positions of removed relevant accounts within the similarity-ranked list are noted to quantify recommendation effectiveness.\n",
    "- These positions are also compared to a popularity-based ranking, which serves as a baseline.\n",
    "\n",
    "Two key metrics are computed:\n",
    "- **Reciprocal Rank**: Indicates how highly the removed relevant accounts are ranked.\n",
    "- **Average Precision (AP)**: Measures precision across ranks, evaluating the accuracy of recommendations.\n",
    "\n",
    "#### Results Compilation\n",
    "- A comprehensive report DataFrame is generated, documenting:\n",
    "  - Individual user analyses\n",
    "  - Rankings of removed accounts\n",
    "  - Reciprocal ranks\n",
    "  - Average precision scores\n",
    "\n",
    "This structured approach enables an effective comparison between similarity-based recommendations and conventional popularity-driven methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7738e9d",
   "metadata": {},
   "source": [
    "num_of_accounts = 10000\n",
    "df_subset_users = df_all.head(num_of_accounts)\n",
    "\n",
    "def calc_similarities(df_subset_users, relevant_accounts):\n",
    "    df_report = pd.DataFrame()\n",
    "    \n",
    "    relevant_account_list = relevant_accounts['twitter_user_id'].values\n",
    "    \n",
    "    for i, user in df_subset_users.head(num_of_accounts).iterrows():\n",
    "        user_list = user['friends_ids']\n",
    "        user_list_subset = list(set(user_list) - set(relevant_account_list))\n",
    "        removed_accounts = set(user_list)-set(user_list_subset)\n",
    "        if removed_accounts == 0:\n",
    "            continue\n",
    "        removed_accounts_name = [df_rec[df_rec['twitter_user_id']==x].iloc[0]['twitter_name'] for x in removed_accounts]\n",
    "        \n",
    "            \n",
    "        if len(removed_accounts_name) > 0:\n",
    "            user_sv = sv.get_average_embeddings(user_list_subset)[0]\n",
    "\n",
    "            df_similarity_results = pd.DataFrame()\n",
    "\n",
    "            for account in relevant_account_list:\n",
    "                try:\n",
    "                    sim = cosine(sv[int(account)],user_sv)\n",
    "                except:\n",
    "                    print(f\"Account {account} type:{type(account)} missing\")\n",
    "                    assert \"\"\n",
    "                df_similarity_results = pd.concat([df_similarity_results, \n",
    "                          pd.DataFrame.from_dict({'account': [account],\n",
    "                                'similarity':[sim]})])\n",
    "            #except Exception as e:\n",
    "                        #    print(e)\n",
    "\n",
    "            sorted_recommendation = pd.merge(df_similarity_results.sort_values(by='similarity',ascending=True),\n",
    "                 relevant_accounts[['twitter_user_id','twitter_screen_name','twitter_name']], left_on='account',right_on='twitter_user_id')\n",
    "            \n",
    "            popular_recommendation = relevant_accounts.sort_values(by='followers_count', ascending=False).reset_index(drop=True)\n",
    "            \n",
    "            removed_accounts_positions = []\n",
    "            popular_accounts_positions = []\n",
    "            \n",
    "            for item in removed_accounts:\n",
    "                removed_accounts_positions.append(sorted_recommendation[sorted_recommendation['account']==item].index[0]+1)\n",
    "                popular_accounts_positions.append(relevant_accounts[relevant_accounts['twitter_user_id']==item].index[0]+1)\n",
    "\n",
    "            print(f\"removed accounts: {removed_accounts_name}\\n-- reommendation similarity positions: {sorted(removed_accounts_positions)} out of {len(sorted_recommendation)}\")\n",
    "            print(f\"-- popular positions: {popular_accounts_positions}\")\n",
    "            \n",
    "            sv_reciprocal_rank = sum([1/acc for acc in removed_accounts_positions]) / len(removed_accounts_positions)\n",
    "            pop_reciprocal_rank = sum([1/acc for acc in popular_accounts_positions]) / len(removed_accounts_positions)\n",
    "            \n",
    "            # Compute precision at each relevant position\n",
    "            precisions = []\n",
    "            for i, rank in enumerate(removed_accounts_positions, start=1):\n",
    "                precisions.append(i / rank)\n",
    "            sv_ap = sum(precisions) / len(removed_accounts_positions) if removed_accounts_positions else 0\n",
    "            \n",
    "            precisions = []\n",
    "            for i, rank in enumerate(popular_accounts_positions, start=1):\n",
    "                precisions.append(i / rank)\n",
    "            pop_ap = sum(precisions) / len(removed_accounts_positions) if removed_accounts_positions else 0\n",
    "            \n",
    "            print(f\"num_of_removed_accounts: {len(removed_accounts_positions)}\")\n",
    "            print(f\"sv_ap:  {sv_ap:.03f}\")\n",
    "            print(f\"pop_ap: {pop_ap:.03f}\")\n",
    "            \n",
    "            df_report = pd.concat([df_report,\n",
    "                                  pd.DataFrame.from_dict({\n",
    "                                      'twitter_user':[user['screen_name']],\n",
    "                                      'removed_accounts': [removed_accounts_name],\n",
    "                                      'similarity_rank': [removed_accounts_positions],\n",
    "                                      'sv_reciprocal_rank': [sv_reciprocal_rank],\n",
    "                                      'pop_reciprocal_rank': [pop_reciprocal_rank],\n",
    "                                      'sv_ap': [sv_ap],\n",
    "                                      'pop_ap': [pop_ap],\n",
    "                                  })])\n",
    "            if df_report.shape[0]>=1000:\n",
    "                break\n",
    "            \n",
    "    return df_report\n",
    "\n",
    "    \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2f1297cf",
   "metadata": {},
   "source": [
    "report = calc_similarities(df_subset_users, relevant_accounts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14ee3424",
   "metadata": {},
   "source": [
    "report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae44a958",
   "metadata": {},
   "source": [
    "sheet_names = ['cars']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b341f01",
   "metadata": {},
   "source": [
    "mrr_report = pd.DataFrame()\n",
    "for category in sheet_names:\n",
    "    \n",
    "    # Read the relevant accounts\n",
    "    df_rec = pd.read_excel(\"popular_accounts_1.xlsx\", sheet_name=category)\n",
    "    relevant_accounts = df_rec[df_rec['use']==1]\n",
    "\n",
    "    users_to_remove = []\n",
    "\n",
    "    for i, ac in relevant_accounts.iterrows():\n",
    "        try:\n",
    "            sv[ac['twitter_user_id']]\n",
    "        except:\n",
    "            users_to_remove.append(ac['twitter_user_id'])\n",
    "\n",
    "\n",
    "    relevant_accounts = relevant_accounts[~relevant_accounts['twitter_user_id'].isin(users_to_remove)]\n",
    "\n",
    "    report = calc_similarities(df_subset_users, relevant_accounts)\n",
    "\n",
    "    mrr_report = pd.concat([mrr_report,\n",
    "                report.describe().loc['mean'].to_frame().rename(columns={'mean': f'{category}_mean'}).T\n",
    "                       ])\n",
    "mrr_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ac553aad",
   "metadata": {},
   "source": [
    "sorted_recommendation[sorted_recommendation['account']==item]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7dfce151",
   "metadata": {},
   "source": [
    "# Sample 20 users per account"
   ]
  },
  {
   "cell_type": "code",
   "id": "38b45cec",
   "metadata": {},
   "source": [
    "topics = pd.ExcelFile(\"popular_accounts_1.xlsx\").sheet_names\n",
    "mrr_report = pd.DataFrame()\n",
    "\n",
    "for topic in topics:\n",
    "    # Read the relevant accounts\n",
    "    print(topic)\n",
    "    df_rec = pd.read_excel(\"popular_accounts_1.xlsx\", sheet_name=topic)\n",
    "    relevant_accounts = df_rec[df_rec['use']==1]\n",
    "    mrr_detailed_report = pd.DataFrame()\n",
    "    \n",
    "    for i, account in tqdm(relevant_accounts.iterrows(), total = relevant_accounts.shape[0]):\n",
    "        \n",
    "        account_to_remove = int(account['twitter_user_id'])\n",
    "        try:\n",
    "            users_to_evaluate = df_all[df_all['friends_ids'].apply(lambda x: account_to_remove in x)].sample(n=50, random_state=1000)\n",
    "        except:\n",
    "            print(f\"{account['twitter_name']} - no followers\")\n",
    "            assert \"\"\n",
    "        #users_to_evaluate['friends_ids_subset'] = users_to_evaluate['friends_ids'].apply(lambda x: list(set(x)-set([account_to_remove])))\n",
    "        \n",
    "        report = calc_similarities(users_to_evaluate, relevant_accounts)\n",
    "\n",
    "        mrr_detailed_report = pd.concat([mrr_detailed_report,\n",
    "                report.describe().loc['mean'].to_frame().rename(columns={'mean': f'{topic}_{account[\"twitter_name\"]}'}).T\n",
    "                       ])\n",
    "    mrr_report = pd.concat([mrr_report,\n",
    "                            mrr_detailed_report.describe().loc['mean'].to_frame().rename(columns={'mean': f'{topic}'}).T\n",
    "               ])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cfdde746",
   "metadata": {},
   "source": [
    "# Calc Similarities with precision"
   ]
  },
  {
   "cell_type": "code",
   "id": "9dc05d49",
   "metadata": {},
   "source": [
    "num_of_accounts = 10000\n",
    "df_subset_users = df_all.head(num_of_accounts)\n",
    "\n",
    "def calc_similarities2(df_subset_users, relevant_accounts):\n",
    "    df_report = pd.DataFrame()\n",
    "    \n",
    "    relevant_account_list = relevant_accounts['twitter_user_id'].values\n",
    "    \n",
    "    for i, user in df_subset_users.head(num_of_accounts).iterrows():\n",
    "        user_list = user['friends_ids']\n",
    "        user_list_subset = list(set(user_list) - set(relevant_account_list))\n",
    "        removed_accounts = set(user_list)-set(user_list_subset)\n",
    "        if removed_accounts == 0:\n",
    "            continue\n",
    "        removed_accounts_name = [df_rec[df_rec['twitter_user_id']==x].iloc[0]['twitter_name'] for x in removed_accounts]\n",
    "        \n",
    "            \n",
    "        if len(removed_accounts_name) > 0:\n",
    "            user_sv = sv.get_average_embeddings(user_list_subset)[0]\n",
    "\n",
    "            df_similarity_results = pd.DataFrame()\n",
    "\n",
    "            for account in relevant_account_list:\n",
    "                try:\n",
    "                    sim = cosine(sv[int(account)],user_sv)\n",
    "                except:\n",
    "                    print(f\"Account {account} type:{type(account)} missing\")\n",
    "                    assert \"\"\n",
    "                df_similarity_results = pd.concat([df_similarity_results, \n",
    "                          pd.DataFrame.from_dict({'account': [account],\n",
    "                                'similarity':[sim]})])\n",
    "            #except Exception as e:\n",
    "                        #    print(e)\n",
    "\n",
    "            sorted_recommendation = pd.merge(df_similarity_results.sort_values(by='similarity',ascending=True),\n",
    "                 relevant_accounts[['twitter_user_id','twitter_screen_name','twitter_name']], left_on='account',right_on='twitter_user_id')\n",
    "            \n",
    "            popular_recommendation = relevant_accounts.sort_values(by='followers_count', ascending=False).reset_index(drop=True)\n",
    "            \n",
    "            removed_accounts_positions = []\n",
    "            popular_accounts_positions = []\n",
    "            \n",
    "            for item in removed_accounts:\n",
    "                removed_accounts_positions.append(sorted_recommendation[sorted_recommendation['account']==item].index[0]+1)\n",
    "                popular_accounts_positions.append(relevant_accounts[relevant_accounts['twitter_user_id']==item].index[0]+1)\n",
    "\n",
    "            #print(f\"removed accounts: {removed_accounts_name}\\n-- reommendation similarity positions: {sorted(removed_accounts_positions)} out of {len(sorted_recommendation)}\")\n",
    "            #print(f\"-- popular positions: {popular_accounts_positions}\")\n",
    "            \n",
    "            sv_map = sum(np.arange(1, len(removed_accounts_positions)+1) / np.array(sorted(removed_accounts_positions))) / len(removed_accounts_positions)\n",
    "            pop_map = sum(np.arange(1, len(popular_accounts_positions)+1) / np.array(sorted(popular_accounts_positions))) / len(popular_accounts_positions)\n",
    "            \n",
    "            #print(f\"num_of_removed_accounts: {len(removed_accounts_positions)}\")\n",
    "            #print(f\"sv_ap:  {sv_map:.03f}\")\n",
    "            #print(f\"pop_ap: {pop_map:.03f}\")\n",
    "            \n",
    "            df_report = pd.concat([df_report,\n",
    "                                  pd.DataFrame.from_dict({\n",
    "                                      'twitter_user':[user['screen_name']],\n",
    "                                      'removed_accounts': [removed_accounts_name],\n",
    "                                      'similarity_rank': [sorted(removed_accounts_positions)],\n",
    "                                      'popularity_rank': [sorted(popular_accounts_positions)],\n",
    "                                      'sv_map': [sv_map],\n",
    "                                      'pop_map': [pop_map],\n",
    "                                  })])\n",
    "            if df_report.shape[0]>=1000:\n",
    "                break\n",
    "            \n",
    "    return df_report\n",
    "\n",
    "#calc_similarities2(users_to_evaluate.head(1), relevant_accounts)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "889d667c",
   "metadata": {},
   "source": [
    "topics = pd.ExcelFile(\"popular_accounts_2.xlsx\").sheet_names\n",
    "map_report = pd.DataFrame()\n",
    "agg_report = pd.DataFrame()\n",
    "agg_detailed_report = pd.DataFrame()\n",
    "\n",
    "for topic in topics:\n",
    "    # Read the relevant accounts\n",
    "    print(topic)\n",
    "    df_rec = pd.read_excel(\"popular_accounts_2.xlsx\", sheet_name=topic)\n",
    "    relevant_accounts = df_rec[df_rec['use']==1]\n",
    "    map_detailed_report = pd.DataFrame()\n",
    "    \n",
    "    for i, account in tqdm(relevant_accounts.iterrows(), total = relevant_accounts.shape[0]):\n",
    "        \n",
    "        account_to_remove = int(account['twitter_user_id'])\n",
    "        try:\n",
    "            users_to_evaluate = df_all[df_all['friends_ids'].apply(lambda x: account_to_remove in x)].sample(n=50, random_state=1000)\n",
    "        except:\n",
    "            print(f\"{account['twitter_name']} - no followers\")\n",
    "            assert \"\"\n",
    "        #users_to_evaluate['friends_ids_subset'] = users_to_evaluate['friends_ids'].apply(lambda x: list(set(x)-set([account_to_remove])))\n",
    "        \n",
    "        report = calc_similarities2(users_to_evaluate, relevant_accounts)\n",
    "        agg_report = pd.concat([agg_report,report])\n",
    "        \n",
    "        map_detailed_report = pd.concat([map_detailed_report,\n",
    "                report.describe().loc['mean'].to_frame().rename(columns={'mean': f'{topic}_{account[\"twitter_name\"]}'}).T\n",
    "                       ])\n",
    "        agg_detailed_report = pd.concat([agg_detailed_report,map_detailed_report])\n",
    "        \n",
    "    map_report = pd.concat([map_report,\n",
    "                            map_detailed_report.describe().loc['mean'].to_frame().rename(columns={'mean': f'{topic}'}).T\n",
    "               ])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b336fe8",
   "metadata": {},
   "source": [
    "agg_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62b584cf",
   "metadata": {},
   "source": [
    "map_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e0c56a5b",
   "metadata": {},
   "source": [
    "file_path = 'mean_average_percision.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "    map_report.to_excel(writer, sheet_name='MeanAveragePrecision', index=True)\n",
    "    agg_report.to_excel(writer, sheet_name=\"DetailedUsers\", index=False)\n",
    "    agg_detailed_report.to_excel(writer, sheet_name=\"DetailedAccounts\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "39e48dc6",
   "metadata": {},
   "source": [
    "## Add resturants"
   ]
  },
  {
   "cell_type": "code",
   "id": "a633ed51",
   "metadata": {},
   "source": [
    "rest = pd.read_csv('/workspace/code/sv/restaurants.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c4ba3f90",
   "metadata": {},
   "source": [
    "df_rec = pd.merge(rest, df_count, on='twitter_user_id')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eef8a976",
   "metadata": {},
   "source": [
    "for i, row in df_rec.iterrows():\n",
    "    try:\n",
    "        sv[row['twitter_user_id']]\n",
    "    except:\n",
    "        print(row['twitter_screen_name'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4703b57",
   "metadata": {},
   "source": [
    "df_rec.columns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d2966c90",
   "metadata": {},
   "source": [
    "df_rec.sort_values(by='friends_ids', ascending=False, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "964fa0c3",
   "metadata": {},
   "source": [
    "df_rec.rename(columns={'friends_ids':'followers_count'}, inplace=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a37b99a",
   "metadata": {},
   "source": [
    "df_rec[['twitter_screen_name','twitter_user_id','twitter_name','wikidata_link', 'wikidata_qid', 'wikidata_label',\n",
    "       'wikidata_desc','followers_count']]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d57bfeb",
   "metadata": {},
   "source": [
    "relevant_accounts = df_rec.head(20)\n",
    "map_detailed_report = pd.DataFrame()\n",
    "\n",
    "for i, account in tqdm(relevant_accounts.iterrows(), total = relevant_accounts.shape[0]):\n",
    "\n",
    "    account_to_remove = int(account['twitter_user_id'])\n",
    "    try:\n",
    "        users_to_evaluate = df_all[df_all['friends_ids'].apply(lambda x: account_to_remove in x)].sample(n=50, random_state=1000)\n",
    "    except:\n",
    "        print(f\"{account['twitter_name']} - no followers\")\n",
    "        assert \"\"\n",
    "    #users_to_evaluate['friends_ids_subset'] = users_to_evaluate['friends_ids'].apply(lambda x: list(set(x)-set([account_to_remove])))\n",
    "\n",
    "    report = calc_similarities2(users_to_evaluate, relevant_accounts)\n",
    "    #agg_report = pd.concat([agg_report,report])\n",
    "\n",
    "    map_detailed_report = pd.concat([map_detailed_report,\n",
    "            report.describe().loc['mean'].to_frame().rename(columns={'mean': f'{topic}_{account[\"twitter_name\"]}'}).T\n",
    "                   ])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e90bd10e",
   "metadata": {},
   "source": [
    "map_detailed_report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52686ab4",
   "metadata": {},
   "source": [
    "report"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d62df101",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "374.645px",
    "left": "1566.65px",
    "right": "20px",
    "top": "120px",
    "width": "376.989px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
